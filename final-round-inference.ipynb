{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install require packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda install faiss-gpu -c pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "n_gpu = faiss.get_num_gpus()\n",
    "\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install fiftyone\n",
    "!pip install torch\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class BaseModel():\n",
    "    @abstractmethod\n",
    "    def encode_text(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode_image(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        norm = np.finfo(v.dtype).eps\n",
    "    return v/norm\n",
    "\n",
    "class VIT(BaseModel):\n",
    "    def __init__(self):\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/16\", device=\"cpu\")\n",
    "\n",
    "        self.model.eval()\n",
    "        self.input_resolution = self.model.visual.input_resolution\n",
    "        self.context_length = self.model.context_length\n",
    "        self.vocab_size = self.model.vocab_size\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        text_tokens = clip.tokenize([text])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.encode_text(text_tokens).float()\n",
    "\n",
    "        return text_features\n",
    "\n",
    "    def encode_frame(self, path = None):\n",
    "        image = self.preprocess(Image.open(path)).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(image)\n",
    "            \n",
    "        frame_features = image_features[0].cpu().detach().numpy()\n",
    "        # frame_features = normalize(frame_features)\n",
    "        \n",
    "        return frame_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self, img_dir: str, vector_dim: int = 512):\n",
    "        self.img_dir = img_dir\n",
    "        self.dataset = fo.Dataset.from_images_dir(\n",
    "            img_dir, name=None, tags=None, recursive=True)\n",
    "        self.object_dir = None\n",
    "        self.video_feature_dict = {}\n",
    "        # faiss\n",
    "        self.index = faiss.IndexFlatIP(vector_dim)\n",
    "\n",
    "\n",
    "    def add_meta_data_images(self):\n",
    "        # Add video, frameid\n",
    "        print('Adding meta data')\n",
    "        pbar = tqdm(self.dataset)\n",
    "        for sample in pbar:\n",
    "            _, sample['video'], sample['frameid'] = sample['filepath'][:-4].rsplit('/', 2)\n",
    "            sample.save()\n",
    "\n",
    "    def add_object_detection(self, object_dir: str):\n",
    "        self.object_dir = object_dir\n",
    "        print('Adding object detection')\n",
    "        pbar = tqdm(self.dataset)\n",
    "        for sample in pbar:\n",
    "            object_path = os.path.join(object_dir, sample['filepath'][-20:-4] + '.json')\n",
    "            try:\n",
    "                with open(object_path) as jsonfile:\n",
    "                    det_data = json.load(jsonfile)\n",
    "            except:\n",
    "                continue\n",
    "            detections = []\n",
    "            for cls, box, score in zip(det_data['detection_class_entities'], det_data['detection_boxes'], det_data['detection_scores']):\n",
    "                # Convert to [top-left-x, top-left-y, width, height]\n",
    "                boxf = [float(box[1]), float(box[0]), float(box[3]) -\n",
    "                        float(box[1]), float(box[2]) - float(box[0])]\n",
    "                scoref = float(score)\n",
    "\n",
    "                # Only add objects with confidence > 0.4\n",
    "                if scoref > 0.4:\n",
    "                    detections.append(\n",
    "                        fo.Detection(\n",
    "                            label=cls,\n",
    "                            bounding_box=boxf,\n",
    "                            confidence=float(score)\n",
    "                        )\n",
    "                    )\n",
    "            sample[\"object_faster_rcnn\"] = fo.Detections(detections=detections)\n",
    "            sample.save()\n",
    "\n",
    "\n",
    "    def get_keyframe_list(self):\n",
    "        '''\n",
    "            Return:\n",
    "                a dictionary: {\n",
    "                    'video_name': List[keyframe]\n",
    "                }\n",
    "        '''\n",
    "        path_all_keyframe = os.path.join(self.img_dir,'*','*.jpg')\n",
    "\n",
    "        all_keyframe = glob(path_all_keyframe)\n",
    "        video_keyframe_dict = {}\n",
    "\n",
    "        for kf in all_keyframe:\n",
    "            _, vid, kf = kf[:-4].rsplit('/', 2)\n",
    "            if vid not in video_keyframe_dict.keys():\n",
    "                video_keyframe_dict[vid] = [kf]\n",
    "            else:\n",
    "                video_keyframe_dict[vid].append(kf)\n",
    "\n",
    "        for k, v in video_keyframe_dict.items():\n",
    "            video_keyframe_dict[k] = sorted(v)\n",
    "\n",
    "        return video_keyframe_dict\n",
    "\n",
    "\n",
    "    def get_video_list(self):\n",
    "        path_all_video = os.path.join(self.img_dir, '*')\n",
    "        \n",
    "        all_video = glob(path_all_video)\n",
    "        all_video = [v.rsplit('/', 1)[-1] for v in all_video]\n",
    "\n",
    "        return all_video\n",
    "\n",
    "    def extract_vector_features_per_frame(self, features_dir):\n",
    "        self.features_dir = features_dir\n",
    "\n",
    "        video_list = self.get_video_list()\n",
    "        keyframe_list = self.get_keyframe_list()\n",
    "        \n",
    "        print('Extracting key frames')\n",
    "        pbar = tqdm(video_list)\n",
    "        for video_name in pbar:\n",
    "            clip_path = os.path.join(features_dir,  video_name + '.npy')\n",
    "            features = np.load(clip_path)\n",
    "            feature_video_dir = os.path.join(features_dir, video_name)\n",
    "\n",
    "            self.video_feature_dict[video_name] = {}\n",
    "            for i, frameid in enumerate(keyframe_list[video_name]):\n",
    "                self.video_feature_dict[video_name][frameid] = features[i]\n",
    "\n",
    "    def add_clip_embedding(self):\n",
    "        print('Add clip embedding')\n",
    "        pbar = tqdm(self.dataset)\n",
    "        embeddings = []\n",
    "        for sample in pbar:\n",
    "            tokens = sample['filepath'].split('/')\n",
    "            video_name, frame_id = tokens[-2], tokens[-1][:-4]\n",
    "            \n",
    "            clip_embedding = self.video_feature_dict[video_name][frame_id]\n",
    "            clip_embedding = clip_embedding.flatten()\n",
    "            embeddings.append(clip_embedding)\n",
    "\n",
    "            sample.save()\n",
    "        \n",
    "        embeddings = np.stack(embeddings, axis=0)\n",
    "        embeddings = embeddings.astype(np.float32)\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "    def search_queries(self, X, top_k: int):\n",
    "        # convert to numpy array\n",
    "        faiss.normalize_L2(X)\n",
    "        distances, indices = self.index.search(X.astype(np.float32), top_k)\n",
    "        return distances, indices\n",
    "\n",
    "    def export(self, export_dir, distances, indices):\n",
    "        indices = indices.tolist()\n",
    "        mask = np.zeros(len(self.dataset), dtype=bool)\n",
    "        for idx in indices:\n",
    "            mask[idx] = True\n",
    "            \n",
    "        result = self.dataset[mask]\n",
    "        for idx, sample in enumerate(result):\n",
    "            sample['similarity'] = distances[idx]\n",
    "            sample.save()\n",
    "            \n",
    "        result.sort_by('similarity', reverse=False)   \n",
    "        result.export(export_dir=export_dir,\n",
    "                       dataset_type=fo.types.FiftyOneDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_dir = '/run/media/zephy_manjaro/Crucial X6/AIC2022/data/keyframes/'\n",
    "obj_dir = 'data/objects'\n",
    "feature_dir = '/home/zephy_manjaro/workspace/code/projects/AIC2022/AIChallenge2022/data/clipfeatures'\n",
    "vector_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████| 887227/887227 [2.3m elapsed, 0s remaining, 6.7K samples/s]      \n",
      "Adding meta data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6f9e40b81a4e29abb7b4d1edb8593c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/887227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding object detection\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc4f26b0f7a4f4888195cdc58d1a7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/887227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting key frames\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8bd022163f47908a0e05f6a30663e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add clip embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6c295a6f6c49f08f27e61571f0b29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/887227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever = Retriever(img_dir, vector_dim)\n",
    "retriever.add_meta_data_images()\n",
    "retriever.add_object_detection(obj_dir)\n",
    "retriever.extract_vector_features_per_frame(feature_dir)\n",
    "retriever.add_clip_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = VIT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_k_images = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_queries = {\n",
    "    1: 'A motorbike parking lot along a street for an event. Then there are two rows of red and yellow lanterns.',\n",
    "    2: 'A man wears a green shirt. His face painted green. There is a number on his shirt',\n",
    "    3: 'A football match. People are wearing yellow shirt and red shirt',\n",
    "    4: 'Neswspaper with pictures of an old man is wearing a blue shirt and a black glass',\n",
    "    5: 'A yellow statue horse and a yellow statue man'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512)\n",
      "Exporting samples...\n",
      " 100% |██████████████████| 1000/1000 [228.6ms elapsed, 0s remaining, 4.4K docs/s]      \n"
     ]
    }
   ],
   "source": [
    "QUERY_ID = [2]\n",
    "\n",
    "!rm -rf submission\n",
    "\n",
    "text_feature_list = np.array([encoder.encode_text(text).cpu().numpy()[0] for (id, text) in text_queries.items() if id in QUERY_ID])\n",
    "print(text_feature_list.shape)\n",
    "text_id_list = [id for id in QUERY_ID]\n",
    "\n",
    "distances_list, indices_list = retriever.search_queries(text_feature_list, n_top_k_images)\n",
    "for i, (distances, indices) in enumerate(zip(distances_list, indices_list)):\n",
    "    retriever.export('submission/{}_top_k_images'.format(text_id_list[i]), distances, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Similar Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'submission/similar-frame' already exists; export will be merged with existing files\n",
      "Exporting samples...\n",
      " 100% |████████████████████| 200/200 [58.5ms elapsed, 0s remaining, 3.4K docs/s] \n",
      "Importing samples...\n",
      " 100% |█████████████████| 200/200 [12.9ms elapsed, 0s remaining, 15.5K samples/s]      \n",
      "Import complete\n",
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?context=ipython&subscription=a4b95168-7727-452f-8dc1-ddf57fcb41ad\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4062576130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video = 'C02_V0381'\n",
    "image = '001280.jpg'\n",
    "\n",
    "path_image = os.path.join('/run/media/zephy_manjaro/Crucial X6/AIC2022/data/keyframes/', video, image)\n",
    "\n",
    "image_feature_list = np.array([encoder.encode_frame(path_image)])\n",
    "\n",
    "distances_list, indices_list = retriever.search_queries(image_feature_list, 200)\n",
    "for i, (distances, indices) in enumerate(zip(distances_list, indices_list)):\n",
    "    retriever.export('submission/similar-frame', distances, indices)\n",
    "    \n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir='submission/similar-frame',\n",
    "    dataset_type=fo.types.FiftyOneDataset\n",
    ")\n",
    "    \n",
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
